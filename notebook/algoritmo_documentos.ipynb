{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMCXX9CfvZfTXjAi4AcgG0J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jPv37k5O50xx","executionInfo":{"status":"ok","timestamp":1682299991355,"user_tz":180,"elapsed":7615,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}},"outputId":"4c5069e6-7d0b-4c75-f513-5ac33632dd22"},"source":["! pip install numpy git+https://github.com/makcedward/nlpaug.git"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/makcedward/nlpaug.git\n","  Cloning https://github.com/makcedward/nlpaug.git to /tmp/pip-req-build-_q3abzwx\n","  Running command git clone --filter=blob:none --quiet https://github.com/makcedward/nlpaug.git /tmp/pip-req-build-_q3abzwx\n","  Resolved https://github.com/makcedward/nlpaug.git to commit 23800cbb9632c7fc8c4a88d46f9c4ecf68a96299\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (1.22.4)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug==1.1.11) (1.5.3)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug==1.1.11) (2.27.1)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from nlpaug==1.1.11) (4.6.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (3.11.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (4.11.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (4.65.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from gdown>=4.0.0->nlpaug==1.1.11) (1.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug==1.1.11) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.2.0->nlpaug==1.1.11) (2022.7.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (1.26.15)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug==1.1.11) (2.4.1)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests>=2.22.0->nlpaug==1.1.11) (1.7.1)\n","Building wheels for collected packages: nlpaug\n","  Building wheel for nlpaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nlpaug: filename=nlpaug-1.1.11-py3-none-any.whl size=405902 sha256=a89015eb7bd085e4052e3b6e521bf1e3d7038da43ef4e83c3b32546cff624136\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-wvrgi12z/wheels/90/bc/37/e55b295d36cbaaaf8394dbd355d28e033e236d2bcc7cf77f3a\n","Successfully built nlpaug\n","Installing collected packages: nlpaug\n","Successfully installed nlpaug-1.1.11\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i96g00iH59Uz","executionInfo":{"status":"ok","timestamp":1682300002368,"user_tz":180,"elapsed":11023,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}},"outputId":"c58c0ad4-5b88-49bb-d0cf-8c2e82aa03a2"},"source":["! pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"]}]},{"cell_type":"code","source":["! pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yZA0v4Na5v_3","executionInfo":{"status":"ok","timestamp":1682300176799,"user_tz":180,"elapsed":4862,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}},"outputId":"571873d9-487b-43ca-fa2c-d32428357b9a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.98\n"]}]},{"cell_type":"code","source":["! pip install googletrans==3.1.0a0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":592},"id":"zanQj2u66uO5","executionInfo":{"status":"ok","timestamp":1682301809158,"user_tz":180,"elapsed":5554,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}},"outputId":"f5ad43c5-32f7-4697-aa79-522e5069f210"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting googletrans==3.1.0a0\n","  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.9/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.12.7)\n","Requirement already satisfied: idna==2.* in /usr/local/lib/python3.9/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n","Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n","Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.9/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n","Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.9/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n","Requirement already satisfied: hstspreload in /usr/local/lib/python3.9/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.1.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.0)\n","Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.9/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n","Requirement already satisfied: h2==3.* in /usr/local/lib/python3.9/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n","Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.9/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n","Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.9/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16368 sha256=20a1349734d23390026d4b4e2029613bcbaf7e40ed4bca136781930926c94865\n","  Stored in directory: /root/.cache/pip/wheels/ae/e1/6c/5137bc3f35aa130deea71575e165cc4f4f0680a88f3d90a636\n","Successfully built googletrans\n","Installing collected packages: googletrans\n","  Attempting uninstall: googletrans\n","    Found existing installation: googletrans 3.0.0\n","    Uninstalling googletrans-3.0.0:\n","      Successfully uninstalled googletrans-3.0.0\n","Successfully installed googletrans-3.1.0a0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["googletrans"]}}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"SWT4-UNf6EWq"},"source":["Algoritmos Caracteres"]},{"cell_type":"code","source":["text = \"Eu adoro NLP e treinar modelos como BERT!\""],"metadata":{"id":"9o62O6JGhXTw","executionInfo":{"status":"ok","timestamp":1682301819205,"user_tz":180,"elapsed":12,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"BndapZbw6DlC","executionInfo":{"status":"ok","timestamp":1682301825383,"user_tz":180,"elapsed":6186,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}}},"source":["\n","import nlpaug.augmenter.word as naw\n","import nlpaug.flow as naf\n","\n","from nlpaug.util import Method"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y0PiYnkN6eij","executionInfo":{"status":"ok","timestamp":1682300253516,"user_tz":180,"elapsed":4,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}}},"source":["from nlpaug.augmenter.word import WordAugmenter\n","\n","class M2M100BackTranslationAug(WordAugmenter):\n","    \n","\n","    def __init__(self, from_lang='en', to_lang='pt',n_data = 1, num_beams = 10,\n","        name='M2M100BackTranslationAug'):\n","        super().__init__(\n","            action='substitute')\n","        \n","        from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n","\n","        model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\").to('cuda')\n","        tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n","\n","        self.src = from_lang\n","        self.dest = to_lang\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.n_data = n_data\n","        self.num_beams = num_beams\n","\n","    def substitute(self, data, n=1):\n","\n","        self.tokenizer.src_lang = self.src\n","        encoded_hi = self.tokenizer(data, return_tensors=\"pt\", truncation=True, padding=True).to('cuda')\n","        generated_tokens = self.model.generate(**encoded_hi, forced_bos_token_id=self.tokenizer.get_lang_id(self.dest),\n","                                               num_return_sequences = self.n_data, num_beams = self.num_beams)\n","        resultado = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","\n","        self.tokenizer.src_lang = self.dest\n","        encoded_hi2 = self.tokenizer(resultado, return_tensors=\"pt\", truncation=True, padding=True).to('cuda')\n","        generated_tokens = self.model.generate(**encoded_hi2, forced_bos_token_id=self.tokenizer.get_lang_id(self.src))\n","        texto = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n","\n","        return texto"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZtJGajw8G1W","executionInfo":{"status":"ok","timestamp":1682300349284,"user_tz":180,"elapsed":9860,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}}},"source":["import nlpaug.augmenter.word as naw\n","\n","augmenter = M2M100BackTranslationAug(from_lang='pt', to_lang='de', n_data=5, num_beams=10)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rBuwJj5E8G8z","executionInfo":{"status":"ok","timestamp":1682300352139,"user_tz":180,"elapsed":2891,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}},"outputId":"adc7d408-084d-4f44-f647-216737a6e523"},"source":["augmented_texts = augmenter.augment(text, n=3)\n","print(\"Original:\")\n","print(text)\n","print(\"Augmented Texts:\")\n","print(augmented_texts)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (200) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Original:\n","Eu adoro NLP e treinar modelos como BERT!\n","Augmented Texts:\n","[['Eu gosto de NLP e treino modelos como BERT!', 'Eu gosto de NLP e treinar modelos como BERT!', 'Eu gosto de NLP e treino modelos como BERT!', 'Eu gosto de treinar NLP e modelos como BERT!', 'Eu gosto de NLP e modelos de treinamento como BERT!'], ['Eu gosto de NLP e treino modelos como BERT!', 'Eu gosto de NLP e treinar modelos como BERT!', 'Eu gosto de NLP e treino modelos como BERT!', 'Eu gosto de treinar NLP e modelos como BERT!', 'Eu gosto de NLP e modelos de treinamento como BERT!'], ['Eu gosto de NLP e treino modelos como BERT!', 'Eu gosto de NLP e treinar modelos como BERT!', 'Eu gosto de NLP e treino modelos como BERT!', 'Eu gosto de treinar NLP e modelos como BERT!', 'Eu gosto de NLP e modelos de treinamento como BERT!']]\n"]}]},{"cell_type":"markdown","source":["# Parafraseador"],"metadata":{"id":"f-1JeLLW60QX"}},{"cell_type":"code","metadata":{"id":"qJZdWfYy_psk","executionInfo":{"status":"ok","timestamp":1682301825384,"user_tz":180,"elapsed":16,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}}},"source":["from nlpaug.augmenter.word import WordAugmenter\n","class PegasusPharaphraseAugmenterOtherLang(WordAugmenter):\n","    # https://arxiv.org/pdf/1511.06709.pdf\n","    \"\"\"\n","    Augmenter that leverage two translation models for augmentation. For example, the source is English. This\n","    augmenter translate source to German and translating it back to English. For detail, you may visit\n","    https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28\n","    :param str from_model_name: Any model from https://huggingface.co/models?filter=translation&search=Helsinki-NLP. As\n","        long as from_model_name is pair with to_model_name. For example, from_model_name is English to Japanese,\n","        then to_model_name should be Japanese to English.\n","    :param str to_model_name: Any model from https://huggingface.co/models?filter=translation&search=Helsinki-NLP.\n","    :param str device: Default value is CPU. If value is CPU, it uses CPU for processing. If value is CUDA, it uses GPU\n","        for processing. Possible values include 'cuda' and 'cpu'. (May able to use other options)\n","    :param bool force_reload: Force reload the contextual word embeddings model to memory when initialize the class.\n","        Default value is False and suggesting to keep it as False if performance is the consideration.\n","    :param int batch_size: Batch size.\n","    :param int max_length: The max length of output text.\n","    :param str name: Name of this augmenter\n","    >>> import nlpaug.augmenter.word as naw\n","    >>> aug = naw.BackTranslationAug()\n","    \"\"\"\n","\n","    def __init__(self, src_lang='pt'  ,max_length=60,num_beams=10,temperature=1.5, num_return_sequences = 3,\n","        name='PegasusPharaphraseAugmenterOtherLang'):\n","        super().__init__(\n","            action='substitute')\n","        \n","        import torch\n","        from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n","        model_name = 'tuner007/pegasus_paraphrase'\n","        torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","        tokenizer = PegasusTokenizer.from_pretrained(model_name)\n","        model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n","\n","        self.src_lang = 'pt'\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","        self.num_beams = num_beams\n","        self.temperature = temperature\n","        self.torch_device = torch_device\n","        self.num_return_sequences = num_return_sequences\n","\n","    def substitute(self, data, n = 10):\n","      from googletrans import Translator\n","      \n","      translator = Translator()\n","      result = translator.translate(data, src = self.src_lang, dest='en')\n","      \n","      \n","      batch = self.tokenizer(result.text,truncation=True,padding='longest',max_length=self.max_length, return_tensors=\"pt\").to(self.torch_device)\n","      translated = self.model.generate(**batch,max_length=self.max_length,num_beams=self.num_beams, \n","                                       num_return_sequences=self.num_return_sequences, temperature=self.temperature)\n","      tgt_text = self.tokenizer.batch_decode(translated, skip_special_tokens=True)\n","\n","      rs_text = []\n","\n","      for t in tgt_text:\n","        fim = translator.translate(t, src = 'en', dest = self.src_lang)\n","        rs_text.append(fim.text)\n","      \n","      return rs_text"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"9aHnYRMTAo6W","executionInfo":{"status":"ok","timestamp":1682301838165,"user_tz":180,"elapsed":12791,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}}},"source":["aug = PegasusPharaphraseAugmenterOtherLang(src_lang='pt', num_return_sequences = 10)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7NV_EibIA6jA","executionInfo":{"status":"ok","timestamp":1682301843677,"user_tz":180,"elapsed":4248,"user":{"displayName":"Lucas Pellicer","userId":"14189643127010164185"}},"outputId":"55d4a492-d6a2-490b-d719-7f6ded461963"},"source":["augmented_texts = aug.augment(text)\n","print(\"Original:\")\n","print(text)\n","print(\"Augmented Texts:\")\n","print(augmented_texts)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Original:\n","Eu adoro NLP e treinar modelos como BERT!\n","Augmented Texts:\n","[['Gosto de treinar modelos como o BERT.', 'Eu gosto de modelos de treinamento como o BERT.', 'Sou um grande fã de modelos de treinamento como o BERT.', 'Gosto de treinar modelos como o BERT.', 'Adoro treinar modelos como o BERT.', 'O BERT é um modelo de treino que adoro.', 'Eu gosto de modelos de treinamento como o BERT.', 'BERT é um modelo de treino que adoro.', 'BERT é um modelo de treinamento que eu gosto.', 'BERT é um modelo de treinamento que eu gosto.']]\n"]}]},{"cell_type":"code","metadata":{"id":"PSC45vboMYu3"},"source":[],"execution_count":null,"outputs":[]}]}
